{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SeBTfks5w-5g",
        "outputId": "8aad1e34-9744-4511-d113-d1ea55b7ebdf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!pip install faiss-gpu\\nimport faiss'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchsummary import summary\n",
        "import torchvision.models as models\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "import plotly.express as px\n",
        "'''!pip install faiss-gpu\n",
        "import faiss'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVBGbfHEm4B3"
      },
      "outputs": [],
      "source": [
        "class AE_Linear01(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = torch.nn.Sequential(\n",
        "        nn.Linear(32*32, 26 * 26),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(26*26, 512)\n",
        "\n",
        "    )\n",
        "    self.decoder = torch.nn.Sequential(\n",
        "        nn.Linear(512, 26*26),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(26*26, 32*32),\n",
        "        nn.Tanh(),\n",
        "      )\n",
        "  def forward(self,x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "2mUntZkK0mqb",
        "outputId": "bafa1b17-f052-4675-dd5e-269e652525cd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nSHOW IMAGE\\nfor img, label in new_loader:\\n  print(np.transpose(img[0], (1,2,0)).shape)\\n  print(img[0])\\n  print(f'label: {label[0]}')\\n  plt.imshow((img[0].detach().numpy().transpose(1, 2, 0)*255).astype(np.uint8))\\n  plt.show()\\n  break\\n\""
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "SHOW IMAGE\n",
        "for img, label in new_loader:\n",
        "  print(np.transpose(img[0], (1,2,0)).shape)\n",
        "  print(img[0])\n",
        "  print(f'label: {label[0]}')\n",
        "  plt.imshow((img[0].detach().numpy().transpose(1, 2, 0)*255).astype(np.uint8))\n",
        "  plt.show()\n",
        "  break\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSbcPacWV6eh"
      },
      "outputs": [],
      "source": [
        "def load_dog():\n",
        "  !export KAGGLE_USERNAME=\"Name\" && export KAGGLE_KEY=\"Key\" && mkdir -p data_dogs && cd data_dogs && kaggle datasets download -d eward96/dog-breed-images && unzip -n dog-breed-images.zip && rm dog-breed-images.zip\n",
        "  !ls data\n",
        "  data_dir = 'data_dogs'\n",
        "  list_imgs = glob.glob(data_dir + \"/**/*.jpg\")\n",
        "  tc = transforms.Compose([\n",
        "          transforms.Resize((256, 256)),\n",
        "          transforms.ToTensor()\n",
        "      ])\n",
        "\n",
        "  image_datasets = datasets.ImageFolder(data_dir, transform=tc)\n",
        "  dloader = torch.utils.data.DataLoader(image_datasets, batch_size=10, shuffle=True)\n",
        "  print(f\"There are {len(list_imgs)} images in the dataset {data_dir}\")\n",
        "  return ['dogs', dloader]\n",
        "\n",
        "def load_stl10():\n",
        "  tensor_transform = transforms.ToTensor()\n",
        "  # Download the MNIST Dataset\n",
        "  dtwo = datasets.STL10(root = \"./data\",\n",
        "                          download = True,\n",
        "                          transform = tensor_transform) #train = True,\n",
        "\n",
        "  # DataLoader is used to load the dataset\n",
        "  # for training\n",
        "  dloader = torch.utils.data.DataLoader(dataset = dtwo,\n",
        "                                      batch_size = 10,\n",
        "                                      shuffle = True)\n",
        "  return ['STL10', dloader]\n",
        "\n",
        "def load_cifar10():\n",
        "  tensor_transform = transforms.ToTensor()\n",
        "  # Download the MNIST Dataset\n",
        "  dtwo = datasets.CIFAR10(root = \"./data\",\n",
        "                          download = True,\n",
        "                          transform = tensor_transform) #train = True,\n",
        "\n",
        "  # DataLoader is used to load the dataset\n",
        "  # for training\n",
        "  dloader = torch.utils.data.DataLoader(dataset = dtwo,\n",
        "                                      batch_size = 10,\n",
        "                                      shuffle = True)\n",
        "  return ['CIFAR10', dloader]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRRLGWnBVbMN"
      },
      "outputs": [],
      "source": [
        "def load_data(datasets=[load_dog]):\n",
        "  data_loaders = []\n",
        "  for data in datasets:\n",
        "    try:\n",
        "      data_loaders.append(data())\n",
        "    except:\n",
        "      print(f'The function {str(data)} does not exist')\n",
        "\n",
        "    return data_loaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJzZKQayXSbF"
      },
      "outputs": [],
      "source": [
        "OUTPUTS = []\n",
        "def copy_embeddings(m, i, o):\n",
        "  global OUTPUTS\n",
        "  \"\"\"Copy embeddings from the penultimate layer.\n",
        "  \"\"\"\n",
        "  o = o[:, :, 0, 0].detach().numpy()#.tolist()\n",
        "  OUTPUTS.append(o)\n",
        "\n",
        "def copy_embeddings_ae(m, i, o):\n",
        "  global OUTPUTS\n",
        "  \"\"\"Copy embeddings from the penultimate layer.\n",
        "  \"\"\"\n",
        "  o = o[:].detach().numpy()#.tolist()\n",
        "  OUTPUTS.append(o)\n",
        "\n",
        "def load_resnet18():\n",
        "  model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "  layer = model._modules.get('avgpool')\n",
        "  _ = layer.register_forward_hook(copy_embeddings) # Return Embeddings at this layer\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad=False #deactivate backpropagation\n",
        "  model.eval()\n",
        "  return ['resnet18', model]\n",
        "\n",
        "def load_ae():\n",
        "  model = AE_Linear01()\n",
        "  model.load_state_dict(torch.load(\"/content/model_weights.pth\"), strict=False)\n",
        "  layer = model._modules.get('encoder')\n",
        "  _ = layer.register_forward_hook(copy_embeddings_ae) # Return Embeddings at this layer\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad=False #deactivate backpropagation\n",
        "  model.eval()\n",
        "  return ['ae', model]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYz0PSPbdMmA"
      },
      "outputs": [],
      "source": [
        "def load_models(models=[]):\n",
        "  models = [model() for model in models]\n",
        "  return models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Br1NfeCXbqi"
      },
      "outputs": [],
      "source": [
        "def generate_embeddings(dloader, model):\n",
        "\n",
        "  # Generate image's embeddings for all images in dloader and saves\n",
        "  # them in the list outputs\n",
        "  i = 0\n",
        "  labels = []\n",
        "\n",
        "  for X, Y in dloader:\n",
        "    X = X.reshape(-1, 32*32)\n",
        "    _ = model(X)#.cuda().detach().cpu().clone().numpy() #labels.append(\n",
        "    labels.extend([y.item() for y in Y])\n",
        "    if i > 100:\n",
        "      break\n",
        "    else:\n",
        "      i += 1\n",
        "  # flatten list of embeddings to remove batches\n",
        "  global OUTPUTS\n",
        "  list_embeddings = [item for sublist in OUTPUTS for item in sublist]\n",
        "  list_embeddings = torch.Tensor(np.array(list_embeddings))\n",
        "  OUTPUTS = []\n",
        "  print(f'Number of Embeddings: {len(list_embeddings)}')\n",
        "  print(f'Embeddings Dimension: {len(list_embeddings[0])}')\n",
        "  print(f'Number labels:{len(labels)}')\n",
        "  return [list_embeddings, labels]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "McVRfHIWcaBt",
        "outputId": "6a8d62b0-1533-4ee2-b6eb-71ee28424a7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Number of Embeddings: 3060\n",
            "Embeddings Dimension: 512\n",
            "Number labels:1020\n",
            "1020\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-3e5dfbecb782>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mNormalize\u001b[0m \u001b[0mimages\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;31m?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   '''\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-46-3e5dfbecb782>\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m#plot_embeddings(embeddings[0][0], embeddings[0][1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mevaluate_cos_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   '''\n\u001b[1;32m     13\u001b[0m   \u001b[0mNormalize\u001b[0m \u001b[0mimages\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;31m?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-139dc2e57eb6>\u001b[0m in \u001b[0;36mevaluate_cos_similarity\u001b[0;34m(embeddings, labels)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maverage_similarity_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0mgrouped_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0maverage_similarity_in_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasses_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrouped_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-139dc2e57eb6>\u001b[0m in \u001b[0;36mgroup_classes\u001b[0;34m(embeddings, labels)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mgrouped_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m       \u001b[0mgrouped_classes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrouped_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "def pipeline():\n",
        "  # load data\n",
        "  dloaders = load_data(datasets=[load_cifar10])\n",
        "  models = load_models(models=[load_ae])\n",
        "  # generate embeddings\n",
        "  embeddings = [generate_embeddings(dloader[1], model[1])for dloader in dloaders for model in models]\n",
        "  print(len(embeddings[0][1]))\n",
        "  # evaluate embeddings\n",
        "  #plot_embeddings(embeddings[0][0], embeddings[0][1])\n",
        "\n",
        "  evaluate_cos_similarity(embeddings[0][0], embeddings[0][1])\n",
        "  '''\n",
        "  Normalize images before Network?\n",
        "  '''\n",
        "pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PWg0uB39TpL"
      },
      "outputs": [],
      "source": [
        "def plot_embeddings(list_embeddings, labels):\n",
        "  # Reduce Embeddings first by PCA to 50 and than from there with tsne to 2\n",
        "  import matplotlib.pyplot as plt\n",
        "  import seaborn as sns\n",
        "  import pandas as pd\n",
        "  pca_50 = PCA(n_components=50)\n",
        "  pca_result_50 = pca_50.fit_transform(list_embeddings)\n",
        "\n",
        "  tsne = TSNE(n_components=2, verbose=1, random_state=123, )\n",
        "  z = tsne.fit_transform(pca_result_50)\n",
        "\n",
        "  # Get labels to give the points in diagram below colors according to label\n",
        "  all_labels = []\n",
        "  '''for inputs, labels in dloader:\n",
        "      all_labels.append(labels.tolist())'''\n",
        "\n",
        "  df_subset = pd.DataFrame()\n",
        "  df_subset['tsne-2d-one'] = z[:,0]\n",
        "  df_subset['tsne-2d-two'] = z[:,1]\n",
        "\n",
        "  plt.figure(figsize=(16,10))\n",
        "  sns.scatterplot(\n",
        "      x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
        "      hue=labels,\n",
        "      palette=sns.color_palette(\"hls\", 10),\n",
        "      data=df_subset,\n",
        "      legend=\"full\",\n",
        "      alpha=0.3\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEkMdCdYkvhG"
      },
      "outputs": [],
      "source": [
        "def evaluate_precision_recall(embeddings,labels):\n",
        "  def calc_mean_vec(embeddings, labels):\n",
        "    sum_vecs = [[] for _ in range(10)]\n",
        "    mean_vecs = []\n",
        "    for i in range(len(embeddings)):\n",
        "      sum_vecs[labels[i]].append(embeddings[i])\n",
        "\n",
        "    for class_vecs in sum_vecs:\n",
        "      if len(class_vecs) >1:\n",
        "        mean_vecs.append(sum(class_vecs)/len(class_vecs))\n",
        "      else:\n",
        "        mean_vecs.append(torch.tensor(class_vecs))\n",
        "    return mean_vecs\n",
        "\n",
        "  def euclid_dist(embedding,mean_vec):\n",
        "    return np.sqrt(sum((embedding - mean_vec)**2))\n",
        "\n",
        "  def calc_true_lables(embeddings, mean_vecs):\n",
        "    distance_all_classes = [[] for _ in range(len(embeddings))]\n",
        "    i=0\n",
        "    for embedding in embeddings:\n",
        "      for mean_vec in mean_vecs:\n",
        "        dist =  euclid_dist(embedding,mean_vec)\n",
        "        distance_all_classes[i].append(dist)\n",
        "      i +=1\n",
        "\n",
        "    new_labels = []\n",
        "    for example_classes in distance_all_classes:\n",
        "      max_val = max(example_classes)\n",
        "      index = example_classes.index(max_val)\n",
        "      new_labels.append(index) #Index=Label da konsistent\n",
        "    return new_labels\n",
        "\n",
        "  # mean Vec\n",
        "  mean_vecs = calc_mean_vec(embeddings, labels)\n",
        "  # Calc Distance each point to mean vecs\n",
        "  new_labels = calc_true_lables(embeddings, mean_vecs)\n",
        "  score = 0.0\n",
        "  for i in range(len(labels)):\n",
        "    if labels[i] == new_labels[i]:\n",
        "      score +=1\n",
        "  score = score/len(labels) # normalize\n",
        "  print(f'score: {score}')\n",
        "  return score\n",
        "\n",
        "def evaluate_cos_similarity(embeddings,labels):\n",
        "  def group_classes(embeddings, labels):\n",
        "    grouped_classes = [[] for _ in range(10)]\n",
        "    for i in range(len(embeddings)):\n",
        "      grouped_classes[labels[i]].append(embeddings[i])\n",
        "    return grouped_classes\n",
        "\n",
        "  def cos_similarity(vec1,vec2):\n",
        "    return sum(vec1 * vec2)/(np.sqrt(sum(vec1**2))*np.sqrt(sum(vec2**2)))\n",
        "\n",
        "  def classes_similarity(grouped_classes):\n",
        "    similarity_classes = [[] for _ in range(10)]\n",
        "    class_id = 0\n",
        "    for classes in grouped_classes:\n",
        "      id_vec1 = 0\n",
        "      for vec1 in classes:\n",
        "        for vec2 in classes[id_vec1:]:\n",
        "          similarity_classes[class_id].append(cos_similarity(vec1.numpy(),vec2.numpy()))\n",
        "        id_vec1 +=1\n",
        "      class_id += 1\n",
        "    print(len(similarity_classes))\n",
        "    average_similarity_classes = []\n",
        "    for classes in similarity_classes:\n",
        "      average_similarity_classes.append(sum(classes)/len(classes))\n",
        "    return average_similarity_classes\n",
        "\n",
        "  grouped_classes = group_classes(embeddings, labels)\n",
        "\n",
        "  average_similarity_in_classes = classes_similarity(grouped_classes)\n",
        "  average_similarity_in_classes = sum(average_similarity_in_classes)/len(average_similarity_in_classes)\n",
        "  print(average_similarity_in_classes)\n",
        "  return average_similarity_in_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "V9AV2bZ5p1DT"
      },
      "outputs": [],
      "source": [
        "'''list_embeddings = np.array(list_embeddings).astype(np.float32)\n",
        "d = list_embeddings.shape[1]\n",
        "print(list_embeddings.shape)\n",
        "quantizer = faiss.IndexFlatL2(512)   # build the index\n",
        "index = faiss.IndexIVFPQ(quantizer, d, 5, 8, 8)\n",
        "index.train(list_embeddings)\n",
        "print(index.is_trained)\n",
        "index.add(list_embeddings)                  # add vectors to the index\n",
        "print(index.ntotal)\n",
        "k = 4                          # we want to see 4 nearest neighbors\n",
        "D, I = index.search(list_embeddings[:2], k)     # actual search\n",
        "print(D)                   # neighbors of the 5 first queries\n",
        "print(I)                  # neighbors of the 5 last queries'''"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
